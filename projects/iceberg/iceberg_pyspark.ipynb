{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Iceberg catalog\n",
    "\n",
    "An Iceberg catalog contains the Iceberg table data and metadata. \n",
    "There are various implementations of the catalog, including REST and JDBC.\n",
    "\n",
    "In this case, a `hadoop` catalog type is used to define a catalog called `local`.\n",
    "\n",
    "A directory called `spark-warehouse/iceberg` is then created on the local filesystem.\n",
    "\n",
    "Within this directory, Iceberg table metadata and data is stored in a subdirectory for each table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff I've learned\n",
    "\n",
    "## file_paths\n",
    "The metadata JSON files and the snapshot AVRO files contain absolute path references in the case of s3 or Glue. \n",
    "For example: `s3a://bjorn-test-bucket-902439737514/glue-warehouse/iceberg/default.db/users/data/00000-0-0f9846f1-7773-46fe-870d-5336aa0122d3-0-00001.parquet`\n",
    "This will make it impossible to clone an iceberg table's files into another catalog\n",
    "\n",
    "For local catalogs it is a relative path which is fair enough\n",
    "\n",
    "## Adding files\n",
    "Requires the iceberg extensions\n",
    "\n",
    "Uploaded some new parquet files in the same bucket but a different prefix\n",
    "This: doesn't support globbing\n",
    "source_table => '`parquet`.`s3a://bjorn-test-bucket-902439737514/extra-parquet-files/*.parquet`'\n",
    "\n",
    "but this works:\n",
    "source_table => '`parquet`.`s3a://bjorn-test-bucket-902439737514/extra-parquet-files/`'\n",
    "\n",
    "# Create table\n",
    "\n",
    "What if we have an existing non-Iceberg, parquet table and want to convert it into an Iceberg table?\n",
    "\n",
    "First you want to define an Iceberg table for the data.\n",
    "\n",
    "CREATE TABLE ... LIKE ... is not supported.\n",
    "But we could use CREATE TABLE ... AS SELECT ... WHERE 1=0\n",
    "\n",
    "(I've confirmed this approach actually works)\n",
    "\n",
    "\n",
    "Then you may want to copy the parquet files because after you add Icebergs metadata the paths will be absolute.\n",
    "\n",
    "Copy them into the \"data\" folder so they look like Icebergers.\n",
    "\n",
    "What if you call add files on the same dir twice, IE using the data dir?\n",
    "Luckily it throws up:\n",
    "\n",
    "java.lang.IllegalStateException: Cannot complete import because data files to be imported already exist within the target table: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet,s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00001-5-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet,s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00002-6-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet.  This is disabled by default as Iceberg is not designed for multiple references to the same file within the same table.  If you are sure, you may set 'check_duplicate_files' to false to force the import.\n",
    "\n",
    "So if you're bringing over partitions it's better to put them in their own folder.\n",
    "Or if you're bringing over an unpartitioned table you could also put it in its own folder, to avoid issues.\n",
    "\n",
    "# Deleting files\n",
    "Deleting a data file from the underlying storage which is in-use will break the table.\n",
    "Unlike in Hive where you can delete partitions or individual files, you cant do that with Iceberg.\n",
    "\n",
    "ERROR BaseReader: Error reading file(s): s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
    "org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
    "\n",
    "# Overview\n",
    "\n",
    "Iceberg is vry popular, but there is a lot of Parquet data sitting around.\n",
    "\n",
    "How can we migrate?\n",
    "> Iceberg migration options:\n",
    "- Snapshot\n",
    "- Migrate (in place)\n",
    "=> No mechanism to do a file-level copy\n",
    "\n",
    "First you may want to try out Iceberg and then finally switch over.\n",
    "\n",
    "Copy the data into a new table\n",
    "\n",
    "# Another issue\n",
    "\n",
    "If you try to do a snapshot of a table from a non-iceberg catalog it doesn't work:\n",
    "\n",
    "IllegalArgumentException: Cannot create Iceberg table in non-Iceberg Catalog. Catalog 'spark_catalog' was of class 'org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog' but 'org.apache.iceberg.spark.SparkSessionCatalog' or 'org.apache.iceberg.spark.SparkCatalog' are required\n",
    "\n",
    "# Another issue\n",
    "\n",
    "Creating a regular parquet table in an iceberg enabled catalog just creates an Iceberg table anyway.\n",
    "\n",
    "Even if I go into the Glue consonle and manually create a Parquet table in the \"icebrge\" database, and put some data in there, and am able to query it with athena, it doesnt work from spark\n",
    "The Iceberg enabled catalog will lists only iceberg tables...\n",
    "\n",
    "So it's basically impossible to use the SNAPSHOT procedure on a non-iceberg table.\n",
    "\n",
    "# So it seems my only option is:\n",
    "\n",
    " 1 Create empty iceberg\n",
    " 2 Use Add files\n",
    "\n",
    "Indeed this way actually seems to work.\n",
    "\n",
    "Now you've played around with your Iceberg table and are happy.\n",
    "You want to convert your existing table into an Iceberg.\n",
    "\n",
    "The problem is the file paths are absolute so you cannot move the files into the new table location.\n",
    "If you copy the files and then call add files you will have duplicate data.\n",
    "\n",
    "If you move them, the table will break.\n",
    "\n",
    "# Can you create an iceberg table on top of an existing table?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_ACCESS_KEY_ID=ASIA5EHMZTCVCYBYKQNI\n",
      "env: AWS_SECRET_ACCESS_KEY=q9+9KfF6NEEZQgGjJ3+r2uCZNzxn788BMH56xkeX\n",
      "env: AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEAoaCWV1LXdlc3QtMSJHMEUCIQDhqZmgG52g/w00fEwmcYibVlR/eGVipHL05cZik4ZpbQIgIV5mr1XwfllbTljrmwj5aun6sIshoiSdKkxTb8Ivpl0qlwMIExABGgw5MDI0Mzk3Mzc1MTQiDIUk8spE1gESmR12Ryr0AkRpJK8Et73roBto8J9nSWf3C21KmgjCY5ZJxaIDdBsZPbIJ251gS2cH2yq2PDj+hgwolrCkzJlYg4yMTzT8GGyCF4c54hTfm+QqE/jZkq/KszUpWZXy4W6+/vzjXo/32L0jNYStSMTJExoK1ETsICyEdKvkv/wJ3lP+0Ye7L9BsWInaZ1jOdugbxW0jYbYTaWu6guArmZiGHuowxkdxDWqVurh8Yf+52UqBlPYDNduJ8Ab6C3uDRO2qzKgGMf3mpeT9B4kP80Zn6Xx+eD4lLBeDUIDxSSWQIZaPxdUuiS4i3BjPpsPjidN/n77YXHIu/Dmtt1FYmfCivDyrzAipUT0c7vXKVSkDdOhJ9MCNXCKtL+Fs53lMErct/wVP5X9oAciTaBsN1UazmPnxzyCSlgolYQ5kystrXumt+2ZqoDyNTwTAdMSjjgJLBiM6CKL8tKF1Fam0LDFjPoBFp315VcdKDM9k1c+AehGvkXnpaI9TvHQPFTD/xs28BjqmAf+jPz99eAr7DKaM4J/o0L0ieTDktMaS9cKI8F7hQvi8aJn7BE3HkzWAzyJzGIV/y79+1CW9Ts3n1cj2ZFLOQxoxxynqScF3o07+u+T2Jd8Ckf5F2Ns4vhz/WuTYHpHpIYzeFb7ISycVviM6iI8cJ2SR7aAbCuqwUOY5A90+FFVuVR0i7GU8/Np/eQTdxYeUX44JpeLXyIsavhfXMydqAR5MjmBtZGU=\n"
     ]
    }
   ],
   "source": [
    "%env AWS_ACCESS_KEY_ID=ASIA5EHMZTCVCYBYKQNI\n",
    "%env AWS_SECRET_ACCESS_KEY=q9+9KfF6NEEZQgGjJ3+r2uCZNzxn788BMH56xkeX\n",
    "%env AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEAoaCWV1LXdlc3QtMSJHMEUCIQDhqZmgG52g/w00fEwmcYibVlR/eGVipHL05cZik4ZpbQIgIV5mr1XwfllbTljrmwj5aun6sIshoiSdKkxTb8Ivpl0qlwMIExABGgw5MDI0Mzk3Mzc1MTQiDIUk8spE1gESmR12Ryr0AkRpJK8Et73roBto8J9nSWf3C21KmgjCY5ZJxaIDdBsZPbIJ251gS2cH2yq2PDj+hgwolrCkzJlYg4yMTzT8GGyCF4c54hTfm+QqE/jZkq/KszUpWZXy4W6+/vzjXo/32L0jNYStSMTJExoK1ETsICyEdKvkv/wJ3lP+0Ye7L9BsWInaZ1jOdugbxW0jYbYTaWu6guArmZiGHuowxkdxDWqVurh8Yf+52UqBlPYDNduJ8Ab6C3uDRO2qzKgGMf3mpeT9B4kP80Zn6Xx+eD4lLBeDUIDxSSWQIZaPxdUuiS4i3BjPpsPjidN/n77YXHIu/Dmtt1FYmfCivDyrzAipUT0c7vXKVSkDdOhJ9MCNXCKtL+Fs53lMErct/wVP5X9oAciTaBsN1UazmPnxzyCSlgolYQ5kystrXumt+2ZqoDyNTwTAdMSjjgJLBiM6CKL8tKF1Fam0LDFjPoBFp315VcdKDM9k1c+AehGvkXnpaI9TvHQPFTD/xs28BjqmAf+jPz99eAr7DKaM4J/o0L0ieTDktMaS9cKI8F7hQvi8aJn7BE3HkzWAzyJzGIV/y79+1CW9Ts3n1cj2ZFLOQxoxxynqScF3o07+u+T2Jd8Ckf5F2Ns4vhz/WuTYHpHpIYzeFb7ISycVviM6iI8cJ2SR7aAbCuqwUOY5A90+FFVuVR0i7GU8/Np/eQTdxYeUX44JpeLXyIsavhfXMydqAR5MjmBtZGU=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/24 14:54:02 WARN Utils: Your hostname, Bjorns-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.68.102 instead (on interface en0)\n",
      "25/01/24 14:54:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/bjorn/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/bjorn/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/bjorn/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dff8ffc4-8ded-414a-97d5-a0eda70fb26f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.5.2 in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.5.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;14.0.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.4 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.4 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.4 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.56.v20240826 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.56.v20240826 in central\n",
      ":: resolution report :: resolve 2335ms :: artifacts dl 57ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.15.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.14 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.5.2 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.5.4 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.56.v20240826 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.56.v20240826 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.56.v20240826] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   49  |   0   |   0   |   1   ||   48  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dff8ffc4-8ded-414a-97d5-a0eda70fb26f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 48 already retrieved (0kB/62ms)\n",
      "25/01/24 14:54:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "S3_LOCATION = \"bjorn-test-bucket-902439737514/tests\"\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"IcebergLocalDevelopment\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws-bundle:1.5.2,org.apache.spark:spark-hadoop-cloud_2.12:3.5.4') \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "\n",
    "# Configure AWS credentials\n",
    "builder = builder \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "\n",
    "# Configure local_iceberg_catalog\n",
    "builder = builder \\\n",
    "    .config(\"spark.sql.catalog.local_iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local_iceberg_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local_iceberg_catalog.warehouse\", \"local-warehouse\")\n",
    "\n",
    "# Configure s3_iceberg_catalog\n",
    "builder = builder \\\n",
    "    .config(\"spark.sql.catalog.s3_iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.s3_iceberg_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.s3_iceberg_catalog.warehouse\", f\"s3a://{S3_LOCATION}/s3-warehouse/iceberg\")\n",
    "\n",
    "# Configure glue_iceberg_catalog\n",
    "builder = builder \\\n",
    "    .config(\"spark.sql.catalog.glue_iceberg_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.glue_iceberg_catalog.type\", \"glue\") \\\n",
    "    .config(\"spark.sql.catalog.glue_iceberg_catalog.warehouse\", f\"s3a://{S3_LOCATION}/glue-warehouse/iceberg\") \\\n",
    "    .config(\"spark.sql.catalog.glue_iceberg_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "# Verify Spark session creation\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/24 14:54:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             catalog|\n",
      "+--------------------+\n",
      "|glue_iceberg_catalog|\n",
      "|local_iceberg_cat...|\n",
      "|  s3_iceberg_catalog|\n",
      "|       spark_catalog|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# database_name aka schema_name\n",
    "database_name = \"iceberg_test_db\"\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS local_iceberg_catalog.{database_name}\")\n",
    "spark.sql(f\"USE local_iceberg_catalog.{database_name}\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS s3_iceberg_catalog.{database_name}\")\n",
    "spark.sql(f\"USE s3_iceberg_catalog.{database_name}\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS glue_iceberg_catalog.{database_name}\")\n",
    "spark.sql(f\"USE glue_iceberg_catalog.{database_name}\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS spark_catalog.{database_name}\")\n",
    "spark.sql(f\"USE spark_catalog.{database_name}\")\n",
    "\n",
    "# Verify Spark session creation\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and query a regular parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-----------+\n",
      "|      namespace|         tableName|isTemporary|\n",
      "+---------------+------------------+-----------+\n",
      "|iceberg_test_db|users_from_parquet|      false|\n",
      "+---------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a regular Parquet table\n",
    "# Specify the catalog name\n",
    "catalog_name = \"glue_iceberg_catalog\"\n",
    "# Specify the database_name aka schema_name\n",
    "database_name = \"iceberg_test_db\"\n",
    "\n",
    "# This didn't work, it creates an ICEBERG table anyway!\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "#   CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.users (\n",
    "#     id INT,\n",
    "#     name STRING,\n",
    "#     age INT\n",
    "#   ) \n",
    "#   USING PARQUET\n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "# Insert some sample data\n",
    "# spark.sql(f\"\"\"\n",
    "#   INSERT INTO {catalog_name}.{database_name}.users VALUES\n",
    "#     (1, 'Alice', 30),\n",
    "#     (2, 'Bob', 25),\n",
    "#     (3, 'Charlie', 35)\n",
    "# \"\"\")\n",
    "\n",
    "spark.sql(\"USE glue_iceberg_catalog.iceberg_test_db\")\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "# spark.sql(f\"SELECT * FROM {catalog_name}.{database_name}.manual_glue_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and query an Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an Iceberg table\n",
    "\n",
    "# Specify the catalog name\n",
    "catalog_name = \"local_iceberg_catalog\"\n",
    "# Specify the database_name aka schema_name\n",
    "database_name = \"iceberg_test_db\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.users (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    age INT\n",
    "  ) \n",
    "  USING iceberg\n",
    "  \"\"\")\n",
    "\n",
    "# Insert some sample data\n",
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO {catalog_name}.{database_name}.users VALUES\n",
    "    (1, 'Alice', 30),\n",
    "    (2, 'Bob', 25),\n",
    "    (3, 'Charlie', 35)\n",
    "    \"\"\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{database_name}.users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────────────────────────────────────────────────────────────────────────────────────┬─────────────────┬───────────────────┬─────────┬─────────────────┬─────────────────────┬─────────────────────┬───────────────────┬──────────────────────┬─────────────────────┬──────────────────┬─────────────────────┬────────────────────┬───────────────────────────────────────────────────────────────────────────────────────────┐\n",
       "│                                        manifest_path                                        │ manifest_length │ partition_spec_id │ content │ sequence_number │ min_sequence_number │  added_snapshot_id  │ added_files_count │ existing_files_count │ deleted_files_count │ added_rows_count │ existing_rows_count │ deleted_rows_count │                                        partitions                                         │\n",
       "│                                           varchar                                           │      int64      │       int32       │  int32  │      int64      │        int64        │        int64        │       int32       │        int32         │        int32        │      int64       │        int64        │       int64        │ struct(contains_null boolean, contains_nan boolean, lower_bound blob, upper_bound blob)[] │\n",
       "├─────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────┼───────────────────┼─────────┼─────────────────┼─────────────────────┼─────────────────────┼───────────────────┼──────────────────────┼─────────────────────┼──────────────────┼─────────────────────┼────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────┤\n",
       "│ local-warehouse/iceberg/default/users/metadata/f0adebed-c455-42bd-b017-0757b65045d5-m0.avro │            6801 │                 0 │       0 │               1 │                   1 │ 3282714786506435414 │                 3 │                    0 │                   0 │                3 │                   0 │                  0 │ []                                                                                        │\n",
       "└─────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────┴───────────────────┴─────────┴─────────────────┴─────────────────────┴─────────────────────┴───────────────────┴──────────────────────┴─────────────────────┴──────────────────┴─────────────────────┴────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "duckdb.sql(\"\"\" INSTALL avro FROM community; LOAD avro; \"\"\")\n",
    "duckdb.sql(f\"\"\" FROM read_avro('{local_warehouse_directory}/iceberg/{schema}/users/metadata/snap*.avro'); \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets test adding new files ot an Iceberg.\n",
    "\n",
    "# First generate some local data\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  INSERT INTO local_catalog.default.users VALUES\n",
    "    (4, 'Dhiya', 20),\n",
    "    (5, 'Elian', 50),\n",
    "    (6, 'Freya', 50)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬─────────┬───────┐\n",
       "│  id   │  name   │  age  │\n",
       "│ int32 │ varchar │ int32 │\n",
       "├───────┼─────────┼───────┤\n",
       "│     4 │ Dhiya   │    20 │\n",
       "│     5 │ Elian   │    50 │\n",
       "│     6 │ Freya   │    50 │\n",
       "└───────┴─────────┴───────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(f\"\"\" FROM read_parquet('local-warehouse/iceberg/default/users/data/*4a6*.parquet'); \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[added_files_count: bigint, changed_partition_count: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create a table for the extra files\n",
    "\n",
    "spark.sql(f\"\"\"\\\n",
    "CALL {catalog}.system.add_files(\n",
    "table => '{schema}.users',\n",
    "source_table => '`parquet`.`s3a://bjorn-test-bucket-902439737514/extra-parquet-files/`'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  4|Dhiya| 20|\n",
      "|  5|Elian| 50|\n",
      "|  6|Freya| 50|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a spark table over the data\n",
    "# First try with a temp table\n",
    "\n",
    "df = spark.read.parquet('s3a://bjorn-test-bucket-902439737514/extra-parquet-files/*.parquet')\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "spark.sql(\"\"\" SELECT * FROM users \"\"\").show()\n",
    "\n",
    "# Create an empty Iceberg table\n",
    "spark.sql(\"\"\"\\\n",
    "CREATE TABLE s3_catalog.default.extra_users\n",
    "USING iceberg\n",
    "AS SELECT * FROM users WHERE 1=0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT * FROM s3_catalog.default.extra_users \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we copy across all the data files\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "bucket = 'bjorn-test-bucket-902439737514'\n",
    "\n",
    "files = s3.Bucket(bucket).objects.filter(Prefix='extra-parquet-files/')\n",
    "\n",
    "for file in files:\n",
    "    key = file.key\n",
    "    if not key.endswith('.parquet'):\n",
    "        continue\n",
    "\n",
    "    object_name = key.split(\"/\")[-1]\n",
    "\n",
    "    copy_source = {\n",
    "        'Bucket': bucket,\n",
    "        'Key': key\n",
    "    }\n",
    "    s3.meta.client.copy(copy_source, bucket, 's3-warehouse/iceberg/default/extra_users/data/extra-parquet-files/' + object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  4|Dhiya| 20|\n",
      "|  5|Elian| 50|\n",
      "|  6|Freya| 50|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT * FROM s3_catalog.default.extra_users \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[added_files_count: bigint, changed_partition_count: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\\\n",
    "CALL {catalog}.system.add_files(\n",
    "table => '{schema}.extra_users',\n",
    "source_table => '`parquet`.`s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/extra-parquet-files/`'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/17 16:51:03 ERROR BaseReader: Error reading file(s): s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n",
      "\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n",
      "\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n",
      "\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n",
      "\tat scala.Option.exists(Option.scala:376)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n",
      "\t... 38 more\n",
      "25/01/17 16:51:03 ERROR Executor: Exception in task 0.0 in stage 64.0 (TID 1642)\n",
      "org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n",
      "\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n",
      "\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n",
      "\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n",
      "\tat scala.Option.exists(Option.scala:376)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n",
      "\t... 38 more\n",
      "25/01/17 16:51:03 WARN TaskSetManager: Lost task 0.0 in stage 64.0 (TID 1642) (192.168.68.104 executor driver): org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n",
      "\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n",
      "\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n",
      "\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n",
      "\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n",
      "\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n",
      "\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n",
      "\tat scala.Option.exists(Option.scala:376)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n",
      "\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n",
      "\t... 38 more\n",
      "\n",
      "25/01/17 16:51:03 ERROR TaskSetManager: Task 0 in stage 64.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o141.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 1642) (192.168.68.104 executor driver): org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n\tat scala.Option.exists(Option.scala:376)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n\t... 38 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n\tat scala.Option.exists(Option.scala:376)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n\t... 38 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43m SELECT * FROM s3_catalog.default.extra_users \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o141.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 1642) (192.168.68.104 executor driver): org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n\tat scala.Option.exists(Option.scala:376)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n\t... 38 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.iceberg.exceptions.NotFoundException: File does not exist: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:164)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.getStat(HadoopInputFile.java:200)\n\tat org.apache.iceberg.parquet.ParquetIO.file(ParquetIO.java:51)\n\tat org.apache.iceberg.parquet.ReadConf.newReader(ReadConf.java:238)\n\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:81)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.init(VectorizedParquetReader.java:90)\n\tat org.apache.iceberg.parquet.VectorizedParquetReader.iterator(VectorizedParquetReader.java:99)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:109)\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:41)\n\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:143)\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120)\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63)\n\tat scala.Option.exists(Option.scala:376)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: No such file or directory: s3a://bjorn-test-bucket-902439737514/s3-warehouse/iceberg/default/extra_users/data/00000-4-4a6dfa15-615f-4814-b6fd-0b52a1f640ed-0-00001.parquet\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3866)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$24(S3AFileSystem.java:3556)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3554)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.lazyStat(HadoopInputFile.java:162)\n\t... 38 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT * FROM s3_catalog.default.extra_users \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 35|\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a regular parquet table in a regular catalog\n",
    "\n",
    "# I had to do this manually....\n",
    "\n",
    "# Create a regular Parquet table\n",
    "# Specify the catalog name\n",
    "catalog_name = \"spark_catalog\"\n",
    "# Specify the database_name aka schema_name\n",
    "database_name = \"iceberg_test_db\"\n",
    "\n",
    "# Creating a regular parquet table in an iceberg enabled catalog just creates an Iceberg table anyway.\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.users_parquet (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    age INT\n",
    "  ) \n",
    "  USING PARQUET\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Insert some sample data\n",
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO {catalog_name}.{database_name}.users_parquet VALUES\n",
    "    (1, 'Alice', 30),\n",
    "    (2, 'Bob', 25),\n",
    "    (3, 'Charlie', 35)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{database_name}.users_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the Iceberg \"snapshot\" procedure\n",
    "\n",
    "in_catalog_name = \"spark_catalog\"\n",
    "out_catalog_name = \"glue_iceberg_catalog\"\n",
    "\n",
    "database_name = \"iceberg_test_db\"\n",
    "\n",
    "# First create an empty Glue Iceberg table\n",
    "spark.sql(f\"\"\"\\\n",
    "CREATE TABLE IF NOT EXISTS {out_catalog_name}.{database_name}.users_from_parquet\n",
    "USING ICEBERG\n",
    "AS SELECT * FROM {in_catalog_name}.{database_name}.users_parquet WHERE 1=0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"SELECT * FROM {out_catalog_name}.{database_name}.users_from_parquet\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot create Iceberg table in non-Iceberg Catalog. Catalog 'spark_catalog' was of class 'org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog' but 'org.apache.iceberg.spark.SparkSessionCatalog' or 'org.apache.iceberg.spark.SparkCatalog' are required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mCALL glue_iceberg_catalog.system.snapshot(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark_catalog.iceberg_test_db.users\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miceberg_test_db.users_snap\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot create Iceberg table in non-Iceberg Catalog. Catalog 'spark_catalog' was of class 'org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog' but 'org.apache.iceberg.spark.SparkSessionCatalog' or 'org.apache.iceberg.spark.SparkCatalog' are required"
     ]
    }
   ],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "# CALL local_iceberg_catalog.system.snapshot('spark_catalog.iceberg_test_db.users', 'iceberg_test_db.users_snap')\n",
    "# \"\"\").show()\n",
    "\n",
    "# IllegalArgumentException: Cannot create Iceberg table in non-Iceberg Catalog. Catalog 'spark_catalog' was of class 'org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog' but 'org.apache.iceberg.spark.SparkSessionCatalog' or 'org.apache.iceberg.spark.SparkCatalog' are required\n",
    "\n",
    "# However if you've created the normal table in an ICEBERG enabled catalog maybe it works?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `glue_iceberg_catalog`.`iceberg_test_db`.`manual_glue_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [glue_iceberg_catalog, iceberg_test_db, manual_glue_table], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# spark.sql(f\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# CALL {catalog_name}.system.snapshot('{database_name}.users_parquet', '{database_name}.users_parquet_snap')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# \"\"\").show()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# IllegalArgumentException: Cannot snapshot a table that isn't in the session catalog (i.e. spark_catalog). Found source catalog: glue_iceberg_catalog\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# apparently this error is because you cant snapshot an Iceberg table, but my table isn't an iceberg table...\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM glue_iceberg_catalog.iceberg_test_db.manual_glue_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/baolsen/projects/iceberg/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `glue_iceberg_catalog`.`iceberg_test_db`.`manual_glue_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [glue_iceberg_catalog, iceberg_test_db, manual_glue_table], [], false\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "# CALL {catalog_name}.system.snapshot('{database_name}.users_parquet', '{database_name}.users_parquet_snap')\n",
    "# \"\"\").show()\n",
    "\n",
    "# org.apache.iceberg.exceptions.NoSuchTableException: Cannot not find source table 'iceberg_test_db.users_parquet'\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# CALL glue_iceberg_catalog.system.snapshot('{database_name}.manual_glue_table', '{database_name}.manual_glue_table_snap')\n",
    "# \"\"\").show()\n",
    "# IllegalArgumentException: Cannot snapshot a table that isn't in the session catalog (i.e. spark_catalog). Found source catalog: glue_iceberg_catalog\n",
    "# apparently this error is because you cant snapshot an Iceberg table, but my table isn't an iceberg table...\n",
    "\n",
    "# \n",
    "# spark.sql(\"SELECT * FROM glue_iceberg_catalog.iceberg_test_db.manual_glue_table\").show()\n",
    "# UnresolvedRelation [glue_iceberg_catalog, iceberg_test_db, manual_glue_table], [], false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {catalog_name}.{database_name}.users_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about creating a regular table in a regular catalog, then making it an Iceberg catalog?\n",
    "\n",
    "catalog_name = \"local_catalog\"\n",
    "# Specify the database_name aka schema_name\n",
    "database_name = \"iceberg_test_db\"\n",
    "\n",
    "# Creating a regular parquet table in an iceberg enabled catalog just creates an Iceberg table anyway.\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS {catalog_name}.{database_name}.users_parquet (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    age INT\n",
    "  ) \n",
    "  USING PARQUET\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Insert some sample data\n",
    "spark.sql(f\"\"\"\n",
    "  INSERT INTO {catalog_name}.{database_name}.users_parquet VALUES\n",
    "    (1, 'Alice', 30),\n",
    "    (2, 'Bob', 25),\n",
    "    (3, 'Charlie', 35)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {catalog_name}.{database_name}.users_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  3|Charlie| 35|\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "+---+-------+---+\n",
      "\n",
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 25|\n",
      "|  3|Charlie| 35|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now that we have an empty Iceberg table and a full Parquet table, lets try to snapshot the Parquet table into the Iceberg table\n",
    "\n",
    "# Normal table\n",
    "spark.sql(f\"SELECT * FROM spark_catalog.{database_name}.users_parquet\").show()\n",
    "# Iceberg table\n",
    "spark.sql(f\"SELECT * FROM glue_iceberg_catalog.{database_name}.users_from_parquet\").show()\n",
    "\n",
    "# Use the ADD FILES command to add the data files to the Iceberg table\n",
    "spark.sql(f\"\"\"\\\n",
    "CALL glue_iceberg_catalog.system.add_files(\n",
    "table => '{database_name}.users_from_parquet',\n",
    "source_table => '`parquet`.`s3a://bjorn-test-bucket-902439737514/tests/manual/`'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM glue_iceberg_catalog.{database_name}.users_from_parquet\").show()\n",
    "\n",
    "# After doing this, the Glue Table Overview panel in AWS would just spin forever, it really doesn't seem to like this.\n",
    "# Okay it came right eventually, but it took a long time.\n",
    "# Athena is able to query the added data at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try create an Iceberg table on top of the manual table data\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS glue_iceberg_catalog.iceberg_test_db.manual_glue_table_iceberg\n",
    "USING iceberg\n",
    "LOCATION 's3a://bjorn-test-bucket-902439737514/tests/manual'\n",
    "          AS SELECT * FROM {in_catalog_name}.{database_name}.users_parquet WHERE 1=0\n",
    "\"\"\")\n",
    "\n",
    "# AS SELECT * FROM glue_iceberg_catalog.iceberg_test_db.manual_glue_table WHERE 1=0\n",
    "# ofc this^ wont work because glue_iceberg_catalog is an Iceberg catalog and doesnt recognse the manual table.\n",
    "\n",
    "# Above CREATE statement works as expected, it creates a metadata folder at the same location as the data.\n",
    "\n",
    "# And ofc there is no data until you do an \"ADD FILES\" command.\n",
    "spark.sql(\"SELECT * FROM glue_iceberg_catalog.iceberg_test_db.manual_glue_table_iceberg\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
